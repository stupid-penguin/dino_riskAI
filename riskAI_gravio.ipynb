{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYkcZLzVTm3a"
      },
      "source": [
        "# Настраиваем окружение"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gradio faiss-cpu langchain_community langchain_gigachat unstructured[all-docs] -q"
      ],
      "metadata": {
        "id": "GZEZfFQEUzg-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjtHF2PQT9oA"
      },
      "source": [
        "# Импортируем библиотеки"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "PdaW5GuuVdzd"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "\n",
        "from langchain_community.document_loaders import UnstructuredPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "from langchain_gigachat.chat_models.gigachat import GigaChat\n",
        "from langchain_community.embeddings.gigachat import GigaChatEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import ConversationalRetrievalChain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcQG5a-zha37"
      },
      "source": [
        "# Инициализируем модели"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "sWfgaWIOheEx"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "giga_auth = userdata.get('token')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LLM GigaChat-Pro\n",
        "giga_pro = GigaChat(model=\"GigaChat-Pro\",\n",
        "                verify_ssl_certs=False,\n",
        "                profanity_check=False,\n",
        "                credentials=giga_auth,\n",
        "                top_p=0.2,\n",
        "                timeout=600)"
      ],
      "metadata": {
        "id": "SqdK5_t5KHKm"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LLM GigaChat-Max\n",
        "giga_max = GigaChat(model=\"GigaChat-Max\",\n",
        "                verify_ssl_certs=False,\n",
        "                profanity_check=False,\n",
        "                credentials=giga_auth,\n",
        "                top_p=0.2,\n",
        "                timeout=600)"
      ],
      "metadata": {
        "id": "mgkaS95TKHRc"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Эмбеддер GigaChat\n",
        "giga_embed = GigaChatEmbeddings(\n",
        "                    scope=\"GIGACHAT_API_PERS\",\n",
        "                    verify_ssl_certs=False,\n",
        "                    credentials=giga_auth)"
      ],
      "metadata": {
        "id": "oZKw9EPMU1XX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsrnfl66cwYT"
      },
      "source": [
        "# Вспомогательные функции"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Jsq22Zq1kufU"
      },
      "outputs": [],
      "source": [
        "def create_conversational_chain(llm, vector_store):\n",
        "    memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "    chain = ConversationalRetrievalChain.from_llm(\n",
        "        llm=llm,\n",
        "        chain_type='stuff',\n",
        "        retriever=vector_store.as_retriever(search_kwargs={\"k\": 4}),\n",
        "        memory=memory,\n",
        "        #verbose=True\n",
        "    )\n",
        "    return chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "hdNl4qZ1c-_N"
      },
      "outputs": [],
      "source": [
        "def create_vector_store(pdf_files):\n",
        "    global giga_embed\n",
        "    text = []\n",
        "\n",
        "    for pdf_file in pdf_files:\n",
        "        pdf_path = pdf_file.name  # Путь загружаемого файла\n",
        "\n",
        "        loader = UnstructuredPDFLoader(pdf_path)\n",
        "        text.extend(loader.load())\n",
        "\n",
        "    # Разделение документа на фрагменты\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000,\n",
        "                                              chunk_overlap=100)\n",
        "    text_chunks = text_splitter.split_documents(text)\n",
        "\n",
        "    # Генерация векторной базы данных\n",
        "    vector_store = FAISS.from_documents(text_chunks, embedding=giga_embed)\n",
        "    vector_store.save_local(\"faiss_index\")\n",
        "    return vector_store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "-Y3Yxw-lbo5t"
      },
      "outputs": [],
      "source": [
        "def upload_pdf(pdf_files):\n",
        "    global vector_store, conversational_chain, giga_max\n",
        "    vector_store = create_vector_store(pdf_files)\n",
        "    if vector_store:\n",
        "        conversational_chain = create_conversational_chain(giga_max, vector_store)\n",
        "        return \"PDF-файл успешно загружен. Задайте мне вопрос!\"\n",
        "    else:\n",
        "        return \"Ошибка при загрузке pdf-файла. Попробуйте ещё раз.\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "lc3unA4hl8Te"
      },
      "outputs": [],
      "source": [
        "def ask_question(question):\n",
        "    if conversational_chain and vector_store:\n",
        "        result = conversational_chain.invoke({\"question\": question, \"chat_history\": []})\n",
        "        return result[\"answer\"]\n",
        "    else:\n",
        "        return \"Пожалуйста, для начала загрузите pdf-файл.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dHjQOnUbpDh"
      },
      "source": [
        "# UI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "v1jZ9teDbuiC"
      },
      "outputs": [],
      "source": [
        "# Creating the Gradio interface\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"## chatbot dino_risk\")\n",
        "\n",
        "    with gr.Row():\n",
        "        pdf_input = gr.File(label=\"Загрузите ваш pdf-файл.\", file_count=\"multiple\", type=\"filepath\")\n",
        "        upload_button = gr.Button(\"Загрузка pdf-файла\")\n",
        "        status_output = gr.Textbox(label=\"Статус загрузки:\", interactive=False)\n",
        "\n",
        "    with gr.Row():\n",
        "        question_input = gr.Textbox(label=\"Задайте вопрос\")\n",
        "        ask_button = gr.Button(\"Спросить\")\n",
        "        answer_output = gr.Textbox(label=\"Ответ ИИ\", interactive=True)\n",
        "\n",
        "    upload_button.click(upload_pdf, inputs=[pdf_input], outputs=[status_output])\n",
        "    ask_button.click(ask_question, inputs=[question_input], outputs=[answer_output])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "demo.launch(debug=True)"
      ],
      "metadata": {
        "id": "bqeIEap8U4Tw"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}